---
title: "BISG Mapping Police Violence"
author: "John A. Curiel"
date: "2024-02-22"
output: html_document
---
# Mapping Police Violence outline 

The goal of this module is to learn how to implement Bayesian Improved Surname Geocoding (BISG) imputation of race, in addition to validation exercises. We shall impute race employing Mapping Police Violence data, which in part records race and location.^[https://mappingpoliceviolence.org/] With these data, we shall cover the general best practices and learn the general error rate. 

## Checking and installing relevant packages

The first step when employing BISG is to install the relevant packages. For BISG, we specifically want the "wru" package ("Who are you?") by Imai and Khanna (2016) in addition to zipWRUext2 by Clark, Curiel and Steelman (2021) for ease of use. Note, because zipWRUext2 relies in part upon the surname dictionary provided by Imai and Khanna's version 0.1-12, the "special install" section in the block of code below shall ensure that the appropriate version is installed, thereby preventing issues with imputation later on. 

```{r setup, include=TRUE, results='asis'}
### check for packages 
pkg <- c("stringr","stringi","tidyverse","foreign","conflicted","data.table","raster","gtools",
         "sp","reshape2","ggplot2","ggpubr","rstudioapi","scales","splitstackshape",
         "tidyr","devtools", "sf", "geojsonio","devtools","wru")

for (i in pkg){
  print(i)
  if(require(i, character.only=TRUE)){
    print(paste(i, "is loaded correctly"))
  } else{
    print(paste("trying to install", i))
    install.packages(i,repos = "http://cran.us.r-project.org")
    if(require(i, character.only=TRUE)){
      print(paste(i, "installed and loaded"))
    } else{
      stop(paste("could not install", i))
    }
  }
}
###special install for wru 

check_wru <- system.file("wru")

require(devtools)
if(require("wru", character.only=TRUE) & as.numeric(substr(packageVersion("wru"),1,3)) < 1 ){
  print(paste("WRU", "is loaded correctly"))
} else{
  print(paste("trying to install wru"))
  if(check_wru!=""){
    detach("package:wru", unload=TRUE)
    print("Detached another installed version of wru")
  }
  install_version("wru", version = "0.1-12", repos = "http://cran.us.r-project.org")
  ### note: if asked to update other versions, just press 3 to skip
  if(require(i, character.only=TRUE)){
    print(paste("wru installed and loaded"))
  } else{
    stop(paste("could not install wru"))
  }
  
}

####pkgs to be used 
library(foreign)
library(tidyverse)
library(dplyr)
library(sp) # basic spatial object handling
library(raster) # raster (pixel) object handling
library(reshape2)
library(stringi) # text processing 
library(stringr) # text processing 
library(splitstackshape)
library(data.table)
library(ggplot2)
library(scales)
library(conflicted)
library(geojsonio)
library(sf)
##install the arealOverlap pkg 
#devtools::install_github("https://github.com/jcuriel-unc/arealOverlap2",subdir="arealOverlap")
#library(arealOverlap)
library(wru)
##install the zipWRUext2 pkg 
devtools::install_github("https://github.com/jcuriel-unc/zipWRUext",subdir="zipWRUext2")
library(zipWRUext2)
## set directory 
data_wd <- dirname(rstudioapi::getActiveDocumentContext()$path)
setwd(data_wd)
substrRight <- function(x, n){
  substr(x, nchar(x)-n+1, nchar(x))
}

```

## Loading in the proper data

The next step is to read in and clean the data. These data were downloaded from the Mapping Police Violence site on October 24, 2022 by Logan Harrah as part of his Senior Capstone Project presented at the Western Political Science Conference on April 7, 2023 titled "I'm Armed, Don't Shoot!" 

We first start by reading in the data "mpv_data.csv" and proceed to clean two fields of interest. First, we clean the field noting the "weapon" reportedly on the person of interest. This, we change to "alleged_weapon."  


```{r data of interest}
### read in the data 
police <- read.csv("data/mpv_df.csv")
## change name for weapon field 
colnames(police)[colnames(police)=="Alleged.Weapon..Source..WaPo.and.Review.of.Cases.Not.Included.in.WaPo.Database."] <-
  "alleged_weapon"
## change weapon field to lower case 
police$alleged_weapon <- str_to_lower(police$alleged_weapon)
```

Next, we proceed to clean the name field, which we will need for the "Surname" portion of BISG. Note, we find that there are 226 names withheld. Further, it appears that the name fields were not properly cleaned. Additionally, the police withheld a number of names. We will need to ensure proper cleaned names for the purpose of the surname aspect of BISG.  

```{r checking name fields, results='asis',include=TRUE}

head(police[,1:5], 10)

```

Therefore, we will clean the names in three stages. First, we shall first note in a dummy field titled "name_withheld" whether a name is absent, coded 1 if true, 0 otherwise. We then shall replace the name with the generic "Noah A Smith", which according to multiple surname and first name dictionaries approximately matches the naive prior employed across multiple BISG associated packages. Such information will be useful later once we seek to identify error rates. Thankfully, it appears that the police withheld only 226 names out of 9,942 observations. 


```{r name checking, include=TRUE,results='asis'}
### we will want a cleaner version to split of full_name 
police$full_name <- police$Victim.s.Full.name

### note: some names are withheld by police 
print(length(which(police$full_name=="Name withheld by police"))) ### note: 226 names withheld

# we will now want to create a dummy variable for later, since these will be information where we rely primarily on the geographic component. 
police$name_withheld <- 0
police$name_withheld[police$full_name=="Name withheld by police"] <- 1
sum(police$name_withheld)

### We will now proceed to choose a name that acts as a naive prior, "Smith". Note, the "Noah A" portion was likewise found to have a naive distribution as validated against the python library, zrp -- Zestai Race Predictor.  
police$full_name[police$full_name=="Name withheld by police"] <- "Noah A Smith"
#table(police$full_name)
length(unique(police$full_name)) # 9662
```

We will next need to clean and split the full name into multiple fields. We will want to be careful, as we only need the surname for today, though it is possible to use first and middle names as well with more advanced packages. Additionally, we saw earlier names such as "Flowers II." Where the suffix is not helpful in imputing race, splitting names might result in losing the surname and only grabbing the suffix. Therefore, we will likewise need to clean the suffix characters out of the full name prior to splitting. The first step is to remove punctuation, for the purpose of ease of cleaning. Next, proceed to replace the suffixes. Next, find the length of the name field, followed by the removal of white space. Commented out is a command to print out a subset of data with names longer than 3 sections. When glancing over the data, there appears to be no indication of any suffix that might thwart the BISG process.  

```{r suffix cleaning, include=TRUE,results='asis'}
#### splitting names and pre-cleaning #####

### get rid of punctuation 
police$full_name <- gsub("[[:punct:]]", " ", police$full_name ) 

## get rid of various suffixes 
police$full_name <- str_replace_all(police$full_name, " IV", "")
police$full_name <- str_replace_all(police$full_name, " III", "")
police$full_name <- str_replace_all(police$full_name, " II", "")
police$full_name <- str_replace_all(police$full_name, " Jr", "")
police$full_name <- str_replace_all(police$full_name, " Sr", "")

## create a length field for name; we want to look at those more than 3. 
police$name_length <- lengths(gregexpr("[A-z]\\W+", police$full_name)) + 1L

## remove extra white spaces 
police$full_name <- gsub("\\s+"," ",police$full_name)
police$full_name <- trimws(police$full_name)

### print out long names; glance over if any obvious mistakes 
#long_names <- subset(police, name_length > 3)
#sort(unique(long_names$full_name)) 
## create ID 
police$mpv_id2 <- seq.int(1,nrow(police),by=1)

```

With the suffix values cleaned out, it is now possible to split the names into individual fields. The commands shall string split the data into list objects, grabbing the first value for the first name, last for the last name, and the second from the last for the middle name. Note, the middle name might grab the first name where the length is less than 3. Therefore, these are coded as "A" where length is equal to 2. Note, there are some names far longer. We will not account for this for now, since even with BISG that employs all parts of the name, little predictive value is gained from middle names.  


```{r splitting names,include=TRUE,results='asis'}
### now work on splitting names 
police$first_name <- sapply(strsplit(police$full_name, " "), `[`, 1) # for first name, we are grabbing the portion that is first before the space split. 
# we can likewise see this by going with the "Noah A Smith" example 
sapply(strsplit("Noah A Smith", " "), `[`, 1) #see, pulled Noah as the first name as the first element of a list. 

police$last_name <- sub(".* ", "", police$full_name) # for last name, we are pulling the last portion of the full name 
sub(".* ", "", "Noah A Smith") ## we will see what it does to name here; note, the name "Smith" is grabbed. 
## now, let's check what happens if no middle name? 
sub(".* ", "", "Noah Smith") # still Smith. Therefore, all should be good to go for the surname of interest. 

#police$name_length <- lengths(gregexpr("[A-z]\\W+", police$full_name)) + 1L
summary(police$name_length) #looks like most do not have middle names ; 
### assign middle name of "A" based on if 2 length
police$middle_name <- sapply( strsplit(police$full_name, " "), tail, 1)
police$middle_name[police$name_length==2] <- "A"

head(police$last_name)

```

## The Geographic component 

When employing BISG, we want the geographic information to be preferably at the level of Census Block Group, which is the most precise data that is also updated via 5-year American Community Survey (ACS). In the event that we do not have geocoding, we can rely upon ZIP codes. Where ZIP codes are not an option, finally name alone.^[County data could be used, though for the purpose of this exercise, we see enough of a reduction in missing data that it is inefficient for the purposes of this tutorial, especially since the command is the same as it is for the ZIP code imputation. ] For this step we are going to make use of the MPV data already overlaid onto Census Block Groups via coordinates for all but 74 observations. The process to do so can be found in the appendix. The processes are cut here for time. 

The mpv data -- now titled police_sf -- and an sf object, gets read in. We next read in the cbfs_acs data, which has the 2016-2020 5 year ACS data. Importantly, it has the r_eth fields, which are the proportion of a state's population by race that lives within a Census Block Group. By state, each column sums to 1. Once we get the fields with the FIPS code to match (length of 12, zero-padded on the left), it is possible to conduct the primary BISG command. We employ predict_race_any() from the zipWRUext package. The arguments of interest are 1) the dataframe on which we apply BISG, 2) the ACS data with the racial information mentioned above, and 3) a vector of the column names that will be necessary to match/merge by geographic level.  Upon completion, we get the predicted probabilities for race by observation. 

However, we next see that there are some observations that have missing values. These likely arose because the block groups contained no residents. Therefore, we next sequentially split off and subset data frames and proceed to run the predict_any command again, this time by ZIP code. Upon completing the ZIP code predictions, we see that there are still a dozen or so observations without predictions, so we finally conduct BISG using the base wru command, predict_race, employing only surname. 

```{r geographic component, echo=FALSE}

### read in the police sf data 
police_sf <- readRDS("data/police_sf.rds")

## now read in the 5-year ACS data 
cbfs_acs <- read.csv("data/acs2019bisgframe.csv")
head(cbfs_acs)
summary(nchar(cbfs_acs$Geo_FIPS))
### not all FIP codes are length 12; 0-pad 
cbfs_acs$Geo_FIPS <- str_pad(cbfs_acs$Geo_FIPS, width=12,side="left",pad="0")


##### Prepping the column names to match for the predict_any merge #####
### now, get the names to be the same as the ACS data 
colnames(police_sf)[colnames(police_sf)=="cbg_fips"] <- "Geo_FIPS"
colnames(police_sf)[colnames(police_sf)=="last_name"] <- "surname"
## predict_race_any uses data frames, not sf; therefore, extract 
police_df <- as.data.frame(police_sf)


##### Running BISG via the predict_any cmd (zipWRUext & wru) #####
### now, we should be able to run predict_any
police_df <- predict_race_any(police_df, cbfs_acs, c("Geo_FIPS")) # arguments: df to be imputed on, the df with the demographic data necessary for the BISG process (r_whi,r_bla,r_his,r_asi,r_oth) ; the fields common in both dfs to do the merge


##### Cleaning up the missing data ##### 
### check to ensure no data missing
sum(is.na(police_df$pred_whi)) ## 24 missing; will want to use ZIP code to predict next 

### subset 
police_df_miss <- subset(police_df, is.na(pred_whi)==T)
police_df <- subset(police_df, is.na(pred_whi)==F)

### drop the empty fields 
police_df_miss <- subset(police_df_miss, select=-c(pred_whi,pred_bla,pred_his,pred_asi,pred_oth))

### read in the zip code data 
zcta_acs <- readRDS("data/zcta_acs2016_2020.rds")
colnames(police_df_miss)[colnames(police_df_miss)=="Zipcode"] <- "zcta5"
police_df_miss <- predict_race_any(police_df_miss,zcta_acs,c("zcta5") )
### no failed merges; check data 
sum(is.na(police_df_miss$pred_whi)) # 1 missing observation 

#### read in the other missing data 
police_coord_missing <-readRDS("data/police_coord_missing.rds")

### run zip code bisg 
colnames(police_coord_missing)[colnames(police_coord_missing)=="Zipcode"] <- "zcta5"
colnames(police_coord_missing)[colnames(police_coord_missing)=="last_name"] <- "surname"
police_coord_missing <- predict_race_any(police_coord_missing,zcta_acs,c("zcta5") )
### check missing race data 
sum(is.na(police_coord_missing$pred_whi)) # 13 

##### Getting the dataframes to match in columns #####

### let's create a slimmed version of these data sets so that we can later bind these with fewer problems 
police_df_slim <- subset(police_df, select=c(Geo_FIPS,full_name,Victim.s.age,Victim.s.gender,
                                             Victim.s.race,Date.of.Incident..month.day.year.,
                                             Street.Address.of.Incident,City,State,Zipcode,County,
                                             Agency.responsible.for.death,Cause.of.death,alleged_weapon,
                                             name_withheld,mpv_id2,surname,geometry,pred_whi,pred_bla,
                                             pred_his,pred_asi,pred_oth))
colnames(police_df_slim)[colnames(police_df_slim)=="Zipcode"] <- "zcta5"

### missing dfs 
police_df_miss <- subset(police_df_miss, select=c(Geo_FIPS,full_name,Victim.s.age,Victim.s.gender,
                                             Victim.s.race,Date.of.Incident..month.day.year.,
                                             Street.Address.of.Incident,City,State,zcta5,County,
                                             Agency.responsible.for.death,Cause.of.death,alleged_weapon,
                                             name_withheld,mpv_id2,surname,geometry,pred_whi,pred_bla,
                                             pred_his,pred_asi,pred_oth) )
### now the missing coord data
police_coord_missing$Geo_FIPS = NA
police_coord_missing$geometry = NA
police_coord_missing <- subset(police_coord_missing, select=c(Geo_FIPS,full_name,Victim.s.age,Victim.s.gender,
                                             Victim.s.race,Date.of.Incident..month.day.year.,
                                             Street.Address.of.Incident,City,State,zcta5,County,
                                             Agency.responsible.for.death,Cause.of.death,alleged_weapon,
                                             name_withheld,mpv_id2,surname,geometry,pred_whi,pred_bla,
                                             pred_his,pred_asi,pred_oth) )
### create a categorical var for level of geography used for each 

### now, subset the data to the remaining with
police_df_slim$impute_level = "cbg"
police_df_miss$impute_level = "zcta"
police_coord_missing$impute_level = "zcta"
### now assign missing vals 
police_df_miss$impute_level[is.na(police_df_miss$pred_whi)==T] <- "surname"
police_coord_missing$impute_level[is.na(police_coord_missing$pred_whi)==T] <- "surname"
### now, subset the missing data 
police_df_miss_surname <- subset(police_df_miss, is.na(pred_whi)==T)
police_coord_missing_surname <- subset(police_coord_missing, is.na(pred_whi)==T)
### now remove the missing data from the prior sets 
police_df_miss <- subset(police_df_miss, is.na(pred_whi)==F)
police_coord_missing <- subset(police_coord_missing, is.na(pred_whi)==F)

### run the wru command for surname alone 
police_df_miss_surname <- predict_race(police_df_miss_surname, surname.only = T)
police_coord_missing_surname <- predict_race(police_coord_missing_surname, surname.only = T)

### bind the data 
police_df_miss_surname <- rbind(police_df_miss_surname,police_coord_missing_surname )

### now, fix teh column names 
police_df_miss_surname <- subset(police_df_miss_surname, 
                                 select=-c(pred_whi,pred_bla,pred_his,pred_asi,pred_oth))
## change the names from wru alone 
colnames(police_df_miss_surname)[grepl("pred.", colnames(police_df_miss_surname))] <-
  str_replace_all(colnames(police_df_miss_surname)[grepl("pred.", colnames(police_df_miss_surname))], "pred.", "pred_")
### now combine all of the data 
police_df_slim_final <- rbind(police_df_slim,police_df_miss,police_coord_missing,police_df_miss_surname )

#### good, now save 
saveRDS(police_df_slim_final, "data/police_df_slim_final.rds")

```




# Appendix 

## Spatial operations for overlaying Census Block Groups onto the MPV data

```{r spatial operations appendix}
### check the coordinates present 
(sum(is.na(police$Longitude))/nrow(police))*100 # most actually present; good 0.744317 ; 
(sum(is.na(police$Latitude))/nrow(police))*100 # most actually present; good

### make the police df into an sf frame 
## first, remove the missing data; save for later 
police_coord_missing <- subset(police, is.na(Longitude)==T)
saveRDS(police_coord_missing, "data/police_coord_missing.rds")
## second, remove missing data 
police <- subset(police, is.na(Longitude)==F )

### read in the cbg file 
#cbgs <- st_read("data/cbgs/CB2010.shp")
#cbgs <- subset(cbgs, FIPS!="020160001001")
cbgs <- readRDS("data/cbgs2010sf.rds")
## convert 
police_sf <- st_as_sf(police, coords= c("Longitude","Latitude"), 
                      crs=st_crs(cbgs)) # long = x, lat = y

## try to make valid 
police_sf <- st_make_valid(police_sf)
#cbgs <- st_make_valid(cbgs)
### save the cbg data here 
#saveRDS(cbgs,"data/cbgs2010sf.rds")

### see what happens with this 
test_obj <- st_intersects(police_sf,cbgs)$FIPS
test_obj <- sapply(st_intersects(police_sf,cbgs), function(z) if (length(z)==0) NA_integer_ else z[1])
### what this produces is an integer of 9868, which suggests that it did match onto the CBG file; now we need 
# to ensure that the data 
police_sf$inter_cbg <- test_obj 
police_sf$cbg_fips <- cbgs$FIPS[test_obj]

### save the police sf data 
saveRDS(police_sf,"data/police_sf.rds")



```

## Identifying Race of Police responsible for violence 

The following is code that I created as a means to identify police officers responsible for killing the victims within the data set. Note, the data are very unclean and do not follow best formatting in regards to tracking police violence. The main problem consists in how the data are entered within the "Names.of.Officers.Involved..DRAFT." field. 

```{r pilot police name cleaning}
## because officers sometimes separated by "and" be sure to replace 
police$Names.of.Officers.Involved..DRAFT. <- str_replace_all(police$Names.of.Officers.Involved..DRAFT.,
                                                             "and", ",")
police$Names.of.Officers.Involved..DRAFT. <- str_replace_all(police$Names.of.Officers.Involved..DRAFT.,
                                                             ":", ",")
police$Names.of.Officers.Involved..DRAFT. <- str_replace_all(police$Names.of.Officers.Involved..DRAFT.,
                                                             "/+", ",")
police$Names.of.Officers.Involved..DRAFT. <- str_replace_all(police$Names.of.Officers.Involved..DRAFT.,
                                                             ";", ",")

officer_df <- data.frame()
for (i in 1:nrow(police)) {
  print(i)
  ## grab the list of names if present 
  off_names <- strsplit(police$Names.of.Officers.Involved..DRAFT.[i], ",") 
  off_names <- unlist(off_names)
  ### officer race here 
  off_race <- strsplit(police$Race.of.Officers.Involved..DRAFT.[i], ",")
  off_race <- unlist(off_race)
  ## grab the list of races if present 
  if(identical(off_names, character(0))==T) {
    print("skipping")
  }else{
    new_df <- as.data.frame(cbind(officer_name=off_names, county=police$County[i], 
                                  agency=police$Agency.responsible.for.death[i],mpv_id2=police$mpv_id2[i]))
    if(length(off_race)==0){
      new_df$officer_race <- NA
    }else if(length(off_race) > 0 & length(off_race) <= length(off_names) ){
      new_df$officer_race <- NA
      new_df$officer_race[1:length(off_race)] <- off_race
    }else if(length(off_race) > 0 & length(off_race) > length(off_names)){
      new_df$officer_race <- NA
      new_df$officer_race[1:length(off_names)] <- off_race[1:length(off_names)]
    }
    if(nrow(new_df)>0 & nrow(officer_df)==0){
      officer_df <- new_df
      
    }else if(nrow(new_df)>0 & nrow(officer_df)>0){
      officer_df <- rbind(officer_df, new_df)
    }
    if(exists("new_df")==T){
      rm(new_df)
    }
  }

}

officer_df$officer_name2 <- gsub("\\s*\\([^\\)]+\\)","",as.character(officer_df$officer_name))  
### looks like some of these oddities are cuased by non-clean data for names; let's check for those below a 
# certain character length 
officer_df$name_length <- nchar(officer_df$officer_name2)
## find length under 3 
length(which(officer_df$name_length<4)) # only 102 
summary(officer_df$name_length)

```